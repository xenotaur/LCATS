{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427d9f4f",
   "metadata": {},
   "source": [
    "# Analyze Corpus\n",
    "\n",
    "This notebook analyzes our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8c603",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1a805",
   "metadata": {},
   "source": [
    "Third-party modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ea74f",
   "metadata": {},
   "source": [
    "Switch to the parent directory so paths can resolve and we write to the right directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd().resolve()\n",
    "project_root = cwd.parent if cwd.name == \"notebooks\" else cwd\n",
    "scripts_dir = project_root / \"scripts\"\n",
    "if scripts_dir.is_dir():\n",
    "    if cwd != project_root:\n",
    "        print(f\"Changing working directory from {cwd} to {project_root}\")\n",
    "        os.chdir(project_root)  # Change to the project root directory.\n",
    "print(\"Working directory:\", pathlib.Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5026e",
   "metadata": {},
   "source": [
    "Add imports from within the project (depends on prior cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcats import constants\n",
    "from lcats import stories\n",
    "from lcats import utils\n",
    "\n",
    "from lcats.analysis import survey\n",
    "from lcats.analysis import llm_extractor\n",
    "from lcats.analysis import scenes\n",
    "from lcats.analysis import text_indexing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d773b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "RELOAD_MODULES = [\n",
    "    constants,\n",
    "    stories,\n",
    "    llm_extractor,\n",
    "    scenes,\n",
    "    survey,\n",
    "    text_indexing,\n",
    "    utils,\n",
    "]\n",
    "def reloader():\n",
    "    for module in RELOAD_MODULES:\n",
    "        print(\"Reloading\", module)\n",
    "        reload(module)\n",
    "    print(\"Reloading complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b811a20",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de713a9f",
   "metadata": {},
   "source": [
    "### Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working corpora\n",
    "# CORPORA_ROOT = project_root / \"data\"\n",
    "# Checked-in corpora\n",
    "CORPORA_ROOT = project_root / \"..\" / \"corpora\"\n",
    "CORPORA_ROOT = CORPORA_ROOT.resolve()  # Resolve to absolute path.\n",
    "\n",
    "print(\"Corpora root:\", CORPORA_ROOT)\n",
    "print(\"Corpora top-level directories:\", end=\" \")\n",
    "os.listdir(CORPORA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5708ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stories = survey.find_corpus_stories(CORPORA_ROOT)\n",
    "len(json_stories)\n",
    "print(utils.sml(json_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing just with a sample of 10 stories for speed.\n",
    "# short_stories = stories[:10]  # lol\n",
    "# story_stats, author_stats = survey.compute_corpus_stats(short_stories)\n",
    "story_stats, author_stats = survey.compute_corpus_stats(json_stories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_stats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de024d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_stats.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1414ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = survey.plot_author_stories_vs_tokens(author_stats, annotate_top=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = survey.plot_author_stories_vs_tokens_sns(author_stats, annotate_top=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = survey.plot_tokens_per_story_by_author(story_stats, top_n=24, min_stories=2, rotate_labels=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdff5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = survey.plot_tokens_per_story_by_author_sns(story_stats, top_n=24, min_stories=2, rotate_labels=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf02d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = survey.plot_tokens_per_story_vs_stories(\n",
    "    author_stats, annotate_top=15, log_y=True, jitter=0.05, spread_step=4, x_spread=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7868122",
   "metadata": {},
   "source": [
    "## Scene-Sequel Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d04671",
   "metadata": {},
   "source": [
    "### Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# Where the notebook is executing (absolute, resolved)\n",
    "CURRENT_PATH = pathlib.Path.cwd().resolve()\n",
    "\n",
    "# Project root = formerly parent of notebooks/, now just current dir\n",
    "# PROJECT_ROOT = CURRENT_PATH.parent \n",
    "PROJECT_ROOT = CURRENT_PATH\n",
    "\n",
    "# Local data/output inside the project\n",
    "DEV_CORPUS = (PROJECT_ROOT / \"data\")\n",
    "DEV_OUTPUT = (PROJECT_ROOT / \"output\")\n",
    "\n",
    "# Sibling-level resources (one level up from project root)\n",
    "GIT_CORPUS = (PROJECT_ROOT.parent / \"corpora\")\n",
    "OPENIA_API_KEYS_ENV = (PROJECT_ROOT.parent / \".secrets\" / \"openai_api_keys.env\")\n",
    "\n",
    "def check_path(path: pathlib.Path, description: str) -> None:\n",
    "    if path.exists():\n",
    "        print(f\"Found {description} at: {path}\")\n",
    "    else:\n",
    "        print(f\"Missing {description} from: {path}\")\n",
    "\n",
    "check_path(DEV_CORPUS, \"DEV_CORPUS\")\n",
    "check_path(DEV_OUTPUT, \"DEV_OUTPUT\")\n",
    "check_path(GIT_CORPUS, \"GIT_CORPUS\")\n",
    "check_path(OPENIA_API_KEYS_ENV, \"OPENIA_API_KEYS_ENV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3ca68",
   "metadata": {},
   "source": [
    "## OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fed3542",
   "metadata": {},
   "source": [
    "Get the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9dcd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(OPENIA_API_KEYS_ENV)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ae7cd",
   "metadata": {},
   "source": [
    "Verify that we can get a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4310f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "print(f\"Loaded OpenAI client: {client} with version: {client._version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce7edf",
   "metadata": {},
   "source": [
    "Verify the API is working. This week. And that you have credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46882768",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a starship captain visiting a planet.\"\n",
    ")\n",
    "\n",
    "print(f\"Story generated on: {date.today()}:\")\n",
    "utils.pprint(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a232c6",
   "metadata": {},
   "source": [
    "## Story Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run from within a notebook, the corpora root is two paths up from the notebook's location.\n",
    "CORPORA_ROOT = GIT_CORPUS  # Checked-in corpora\n",
    "# CORPORA_ROOT = DEV_CORPUS  # Command line working corpora\n",
    "\n",
    "# Now load the corpora\n",
    "corpora = stories.Corpora(CORPORA_ROOT)\n",
    "\n",
    "print(\"Loaded corpora:\")\n",
    "print(f\" - root: {corpora.corpora_root}\")\n",
    "print(f\" - corpora: {len(corpora.corpora)}\")\n",
    "print(f\" - stories: {len(corpora.stories)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Example story: corpora.stories[0]:\")\n",
    "example_story = corpora.stories[0]\n",
    "print(f\"Story type: {type(example_story)} with a body of {len(example_story.body)} characters.\")\n",
    "print(example_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f64c3",
   "metadata": {},
   "source": [
    "## Scene and Sequel Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46178a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_extractor = scenes.make_scene_sequel_extractor(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_extraction = scene_extractor.extract(example_story.body)\n",
    "example_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_result = example_extraction['extracted_output']\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaeeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scenes(scenes):\n",
    "    for i, scene in enumerate(scenes):\n",
    "        event_type = scene.get('event_type', 'unknown')\n",
    "        reason = scene.get('reason', 'unknown')\n",
    "        event_text = scene.get('event_text', '')\n",
    "        print(f\"Scene {i}: Type {event_type}\")\n",
    "        print(f\" - Reason: {reason}\")\n",
    "        print(f\" - Text ({len(event_text)} characters): {utils.sm(event_text, limit=100)}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "display_scenes(example_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8bf3a",
   "metadata": {},
   "source": [
    "### Revised Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCENE_SEQUEL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a narrative segmentation assistant. Your job is to segment a story\n",
    "into COARSE-GRAINED, contiguous narrative segments (“scenes” at the level\n",
    "of time/place), then label each segment.\n",
    "\n",
    "### Segment Types\n",
    "- dramatic_scene: a narrative scene where a focal character with a Goal takes\n",
    "  Action, encounters Conflict, and reaches a Disaster or Success (GACD).\n",
    "- dramatic_sequel: a narrative scene (typically after a dramatic_scene) where\n",
    "  a focal character experiences Emotion, reasons about Options, Anticipates\n",
    "  outcomes, and Chooses a new goal (ERAC).\n",
    "- narrative_scene: a narrative scene unified by time/place (and often\n",
    "  character/action) but lacking clear GACD/ERAC structure.\n",
    "- other: text that is not a narrative scene (e.g., front/back matter,\n",
    "  epigraphs, meta-commentary, tables of contents, etc.).\n",
    "\n",
    "### Granularity Rules (VERY IMPORTANT)\n",
    "1) Coarse segmentation only. Prefer FEWER, LARGER segments over many small ones.\n",
    "2) Split primarily on MEANINGFUL changes in TIME and/or PLACE (or explicit\n",
    "   scene-break markers like “***”, chapter headers, clear time jumps).\n",
    "3) Do NOT split simply because a paragraph or a couple of sentences shift topic.\n",
    "   If time/place is stable, keep them in the same segment.\n",
    "4) Merge tiny candidate segments (< ~3 sentences or ~100 characters) into\n",
    "   adjacent segments unless there is an explicit time/place change.\n",
    "5) Dialogue ping-pong alone is not a boundary; treat as one scene unless\n",
    "   time/place changes.\n",
    "6) A dramatic_sequel typically follows a dramatic_scene in the SAME time/place,\n",
    "   unless the text clearly relocates the character in time/place.\n",
    "7) If unsure between dramatic_scene vs narrative_scene, choose narrative_scene.\n",
    "   If unsure between dramatic_sequel vs narrative_scene, choose narrative_scene.\n",
    "\n",
    "### Output Requirements\n",
    "- Output MUST be valid JSON only (no preface or commentary).\n",
    "- Return a single object: { \"segments\": [ ... ] }.\n",
    "- For each segment, include:\n",
    "  - segment_id: integer index starting at 1.\n",
    "  - segment_type: one of \"dramatic_scene\", \"dramatic_sequel\",\n",
    "    \"narrative_scene\", \"other\".\n",
    "  - start_char, end_char: 0-based character offsets into the provided story_text\n",
    "    covering the ENTIRE contiguous segment (Python slicing semantics).\n",
    "  - summary: a short summary (not the full text), ≤ 200 characters.\n",
    "  - cohesion: brief notes identifying the unifying TIME/PLACE/CHARACTERS.\n",
    "  - gacd: for dramatic_scene only (else null): { \"goal\": \"...\", \"action\": \"...\",\n",
    "    \"conflict\": \"...\", \"outcome\": \"Disaster|Success|Unclear\" }.\n",
    "  - erac: for dramatic_sequel only (else null): { \"emotion\": \"...\", \"reason\": \"...\",\n",
    "    \"anticipation\": \"...\", \"choice\": \"...\" }.\n",
    "  - reason: 1–3 sentences explaining the label and boundary choice (focus on\n",
    "    time/place continuity and GACD/ERAC evidence).\n",
    "  - confidence: float in [0,1].\n",
    "\n",
    "Ensure segments are contiguous, non-overlapping, and collectively cover only\n",
    "the parts of the text that are actual narrative (it is okay if front/back matter\n",
    "is labeled as \"other\" and some gaps are unsegmented).\n",
    "\"\"\"\n",
    "\n",
    "SCENE_SEQUEL_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "You will receive a STORY in plain text. Segment it into COARSE narrative\n",
    "segments and label each as dramatic_scene, dramatic_sequel, narrative_scene,\n",
    "or other, following the system instructions.\n",
    "\n",
    "Procedure you MUST follow (internally):\n",
    "1) Skim the STORY to identify major time/place blocks and explicit scene-breaks.\n",
    "2) Propose initial boundaries at major time/place changes or explicit markers.\n",
    "3) Merge adjacent tiny spans (< ~3 sentences or ~100 chars) unless there is a\n",
    "   real time/place shift.\n",
    "4) Classify each final segment:\n",
    "   - dramatic_scene → find GACD evidence (goal, action, conflict, outcome).\n",
    "   - dramatic_sequel → find ERAC evidence (emotion, reason, anticipation, choice).\n",
    "   - narrative_scene → time/place unified but no clear GACD/ERAC.\n",
    "   - other → non-narrative material.\n",
    "5) Produce the JSON described in the system prompt. Use the exact schema and keys.\n",
    "\n",
    "Return ONLY JSON with this shape:\n",
    "{{\n",
    "  \"segments\": [\n",
    "    {{\n",
    "      \"segment_id\": 1,\n",
    "      \"segment_type\": \"dramatic_scene\" | \"dramatic_sequel\" | \"narrative_scene\" | \"other\",\n",
    "      \"start_char\": 0,\n",
    "      \"end_char\": 1234,\n",
    "      \"summary\": \"<≤200-char summary of the segment>\",\n",
    "      \"cohesion\": {{\n",
    "        \"time\": \"<what time unifies this segment (if stated or implied)>\",\n",
    "        \"place\": \"<what place unifies this segment (if stated or implied)>\",\n",
    "        \"characters\": [\"<main character(s)>\"]\n",
    "      }},\n",
    "      \"gacd\": {{\n",
    "        \"goal\": \"...\",\n",
    "        \"action\": \"...\",\n",
    "        \"conflict\": \"...\",\n",
    "        \"outcome\": \"Disaster|Success|Unclear\"\n",
    "      }} | null,\n",
    "      \"erac\": {{\n",
    "        \"emotion\": \"...\",\n",
    "        \"reason\": \"...\",\n",
    "        \"anticipation\": \"...\",\n",
    "        \"choice\": \"...\"\n",
    "      }} | null,\n",
    "      \"reason\": \"<why this label and these boundaries>\",\n",
    "      \"confidence\": 0.0\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "STORY:\n",
    "\\\"\\\"\\\"{story_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def make_scene_sequel_extractor(client):\n",
    "    return llm_extractor.JSONPromptExtractor(\n",
    "        client,\n",
    "        system_prompt=SCENE_SEQUEL_SYSTEM_PROMPT,\n",
    "        user_prompt_template=SCENE_SEQUEL_USER_PROMPT_TEMPLATE,\n",
    "        output_key=\"segments\",\n",
    "        default_model=\"gpt-4o\",\n",
    "        temperature=0.2,\n",
    "        force_json=True,\n",
    "    )\n",
    "\n",
    "revised_extractor = make_scene_sequel_extractor(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11039c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_extraction = revised_extractor.extract(example_story.body)\n",
    "revised_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_result = revised_extraction['extracted_output']\n",
    "revised_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_revised(story_text, extracted_scenes):\n",
    "    for i, scene in enumerate(extracted_scenes):\n",
    "        segment_id = scene.get('segment_id', 'unknown')\n",
    "        segment_type = scene.get('segment_type', 'unknown')\n",
    "        start_char = scene.get('start_char', -1)\n",
    "        end_char = scene.get('end_char', -1)\n",
    "        summary = scene.get('summary', '')\n",
    "        cohesion = scene.get('cohesion', {})\n",
    "        gacd = scene.get('gacd', None)\n",
    "        erac = scene.get('erac', None)\n",
    "        reason = scene.get('reason', 'unknown')\n",
    "        confidence = scene.get('confidence', -1.0)\n",
    "        print(f\"Scene {i}: Type {segment_type} (Confidence: {confidence})\")\n",
    "        print(f\" - Segmentation Rationale: {reason}\")\n",
    "        print(f\" - Summary: {summary}\")\n",
    "        print(f\" - Segment ID: {segment_id}, Chars: [{start_char}:{end_char}], Length: {end_char - start_char} chars\")\n",
    "        print(f\" - Cohesion: {cohesion}\")\n",
    "        if gacd:\n",
    "            print(f\" - GACD: {gacd}\")\n",
    "        if erac:\n",
    "            print(f\" - ERAC: {erac}\")\n",
    "        # scene_text = story_text[start_char:end_char] if 0 <= start_char < end_char <= len(story_text) else ''\n",
    "        # print(f\" - Scene Text:\")\n",
    "        # print(scene_text)\n",
    "        print()\n",
    "\n",
    "display_revised(example_story.body, revised_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_story.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.pprint(example_story.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_SEQUEL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a narrative segmentation assistant. Your job is to segment a story\n",
    "into COARSE-GRAINED, contiguous narrative segments (“scenes” at the level\n",
    "of time/place), then label each segment.\n",
    "\n",
    "### Segment Types\n",
    "- dramatic_scene: a narrative scene where a focal character with a Goal takes\n",
    "  Action, encounters Conflict, and reaches a Disaster or Success (GACD).\n",
    "- dramatic_sequel: a narrative scene (typically after a dramatic_scene) where\n",
    "  a focal character experiences Emotion, reasons about Options, Anticipates\n",
    "  outcomes, and Chooses a new goal (ERAC).\n",
    "- narrative_scene: a narrative scene unified by time/place (and often\n",
    "  character/action) but lacking clear GACD/ERAC structure.\n",
    "- other: text that is not a narrative scene (e.g., front/back matter,\n",
    "  epigraphs, meta-commentary, tables of contents, etc.).\n",
    "\n",
    "### Granularity Rules (VERY IMPORTANT)\n",
    "1) Coarse segmentation only. Prefer FEWER, LARGER segments over many small ones.\n",
    "2) Split primarily on MEANINGFUL changes in TIME and/or PLACE (or explicit\n",
    "   scene-break markers like “***”, chapter headers, clear time jumps).\n",
    "3) Do NOT split simply because a paragraph or a couple of sentences shift topic.\n",
    "   If time/place is stable, keep them in the same segment.\n",
    "4) Merge tiny candidate segments (< ~3 sentences or ~100 characters) into\n",
    "   adjacent segments unless there is an explicit time/place change.\n",
    "5) Dialogue ping-pong alone is not a boundary; treat as one scene unless\n",
    "   time/place changes.\n",
    "6) A dramatic_sequel typically follows a dramatic_scene in the SAME time/place,\n",
    "   unless the text clearly relocates the character in time/place.\n",
    "7) If unsure between dramatic_scene vs narrative_scene, choose narrative_scene.\n",
    "   If unsure between dramatic_sequel vs narrative_scene, choose narrative_scene.\n",
    "\n",
    "### Coverage & Ordering Rules\n",
    "- Ensure coverage across the entire STORY, not just the beginning. If later\n",
    "  paragraphs are narrative but do not clearly fit GACD/ERAC, label them as\n",
    "  narrative_scene (or other) rather than omitting them.\n",
    "- Segments must be in ascending order, contiguous within their own boundaries,\n",
    "  and non-overlapping. It is acceptable to include “other” segments for\n",
    "  non-narrative material.\n",
    "\n",
    "### Output Requirements (JSON ONLY)\n",
    "Return exactly one JSON object: { \"segments\": [ ... ] }\n",
    "\n",
    "For each segment include:\n",
    "- segment_id: integer index starting at 1.\n",
    "- segment_type: \"dramatic_scene\" | \"dramatic_sequel\" | \"narrative_scene\" | \"other\".\n",
    "\n",
    "# --- Robust location selectors (PRIMARY) ---\n",
    "- start_par_id: integer paragraph id where the segment begins (inclusive).\n",
    "- end_par_id: integer paragraph id where the segment ends (inclusive).\n",
    "- start_exact: the FIRST ≤120 characters of the segment, COPIED VERBATIM from the STORY text.\n",
    "- end_exact: the LAST ≤120 characters of the segment, COPIED VERBATIM from the STORY text.\n",
    "- start_prefix: ≤60 characters immediately BEFORE start_exact in the STORY (\"\" if none).\n",
    "- end_suffix: ≤60 characters immediately AFTER end_exact in the STORY (\"\" if none).\n",
    "\n",
    "Rules for anchors:\n",
    "- Copy characters EXACTLY as they appear in the STORY (whitespace/punctuation included).\n",
    "- Do NOT include paragraph id markers like [P0001] in start_exact/end_exact/prefix/suffix.\n",
    "  These markers are scaffolding, not part of the narrative text.\n",
    "\n",
    "# --- Advisory offsets (OPTIONAL) ---\n",
    "- start_char: 0-based start index into the STORY string (Python slicing) or null if unsure.\n",
    "- end_char: 0-based end index (exclusive) into the STORY or null if unsure.\n",
    "\n",
    "# --- Descriptive fields ---\n",
    "- summary: ≤200 characters summarizing the segment (not the full text).\n",
    "- cohesion: brief notes identifying the unifying TIME/PLACE/CHARACTERS.\n",
    "- gacd: for dramatic_scene only, else null:\n",
    "  { \"goal\": \"...\", \"action\": \"...\", \"conflict\": \"...\", \"outcome\": \"Disaster|Success|Unclear\" }.\n",
    "- erac: for dramatic_sequel only, else null:\n",
    "  { \"emotion\": \"...\", \"reason\": \"...\", \"anticipation\": \"...\", \"choice\": \"...\" }.\n",
    "- reason: 1–3 sentences justifying the label and boundary (refer to time/place continuity and GACD/ERAC cues).\n",
    "- confidence: float in [0,1].\n",
    "\"\"\"\n",
    "\n",
    "SCENE_SEQUEL_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "You will receive a STORY with paragraph ids embedded as markers like [P0001].\n",
    "Use paragraph ids for boundaries and supply robust text anchors as described.\n",
    "\n",
    "Procedure you MUST follow (internally):\n",
    "1) Skim the STORY to identify major time/place blocks and explicit scene-breaks.\n",
    "2) Propose initial boundaries at meaningful time/place changes or explicit markers.\n",
    "3) Merge adjacent tiny spans (< ~3 sentences or ~100 chars) unless there is a real time/place shift.\n",
    "4) Classify each final segment:\n",
    "   - dramatic_scene → find GACD evidence (goal, action, conflict, outcome).\n",
    "   - dramatic_sequel → find ERAC evidence (emotion, reason, anticipation, choice).\n",
    "   - narrative_scene → time/place unified but no clear GACD/ERAC.\n",
    "   - other → non-narrative material.\n",
    "5) Ensure later paragraphs are not omitted: if unsure, label as narrative_scene or other.\n",
    "6) Produce ONLY the JSON described in the system prompt, using the exact keys and schema.\n",
    "\n",
    "Return ONLY JSON with this shape:\n",
    "{{\n",
    "  \"segments\": [\n",
    "    {{\n",
    "      \"segment_id\": 1,\n",
    "      \"segment_type\": \"dramatic_scene\" | \"dramatic_sequel\" | \"narrative_scene\" | \"other\",\n",
    "      \"start_par_id\": 1,\n",
    "      \"end_par_id\": 3,\n",
    "      \"start_exact\": \"<first ≤120 chars of this segment, verbatim from STORY>\",\n",
    "      \"end_exact\": \"<last ≤120 chars of this segment, verbatim from STORY>\",\n",
    "      \"start_prefix\": \"<≤60 chars before start_exact or \"\">\",\n",
    "      \"end_suffix\": \"<≤60 chars after end_exact or \"\">\",\n",
    "      \"start_char\": null,\n",
    "      \"end_char\": null,\n",
    "      \"summary\": \"<≤200-char summary>\",\n",
    "      \"cohesion\": {{\n",
    "        \"time\": \"<unifying time (stated or implied)>\",\n",
    "        \"place\": \"<unifying place (stated or implied)>\",\n",
    "        \"characters\": [\"<main character(s)>\"]\n",
    "      }},\n",
    "      \"gacd\": {{\n",
    "        \"goal\": \"...\",\n",
    "        \"action\": \"...\",\n",
    "        \"conflict\": \"...\",\n",
    "        \"outcome\": \"Disaster|Success|Unclear\"\n",
    "      }} | null,\n",
    "      \"erac\": {{\n",
    "        \"emotion\": \"...\",\n",
    "        \"reason\": \"...\",\n",
    "        \"anticipation\": \"...\",\n",
    "        \"choice\": \"...\"\n",
    "      }} | null,\n",
    "      \"reason\": \"<why these boundaries/label>\",\n",
    "      \"confidence\": 0.0\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "STORY (with paragraph ids; DO NOT include [P####] markers in anchors):\n",
    "\\\"\\\"\\\"{indexed_story_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "revised_extractor = llm_extractor.JSONPromptExtractor(\n",
    "    client,\n",
    "    system_prompt=SCENE_SEQUEL_SYSTEM_PROMPT,\n",
    "    user_prompt_template=SCENE_SEQUEL_USER_PROMPT_TEMPLATE,  # expects {indexed_story_text} or {story_text}\n",
    "    output_key=\"segments\",\n",
    "    default_model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    force_json=True,\n",
    "    text_indexer=text_indexing.paragraph_text_indexer,\n",
    "    result_aligner=text_indexing.segments_result_aligner,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_extraction = revised_extractor.extract(example_story.body)\n",
    "revised_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_result = revised_extraction['extracted_output']\n",
    "revised_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_revised(story_text, extracted_scenes):\n",
    "    \"\"\"\n",
    "    Pretty-print segment results produced by the updated extractor.\n",
    "\n",
    "    - Uses start_char/end_char when valid.\n",
    "    - If missing/invalid, derives a best-effort span from start_exact/end_exact.\n",
    "    - Uses utils.sm for compact previews.\n",
    "    - Normalizes preview text:\n",
    "        * collapse runs of spaces to a single space\n",
    "        * single newlines -> spaces\n",
    "        * 2+ newlines -> single newline\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from lcats import utils\n",
    "\n",
    "    def _normalize_preview(s: str) -> str:\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        # unify newlines\n",
    "        s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "        # mark paragraph breaks (2+ newlines)\n",
    "        s = re.sub(r\"\\n{2,}\", \"\\u2029\", s)\n",
    "        # single newlines -> spaces\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "        # collapse spaces/tabs\n",
    "        s = re.sub(r\"[ \\t\\u00A0]+\", \" \", s).strip()\n",
    "        # restore paragraph breaks to single newline\n",
    "        s = s.replace(\"\\u2029\", \"\\n\")\n",
    "        return s\n",
    "\n",
    "    def _sm_norm(s: str, limit: int) -> str:\n",
    "        return utils.sm(_normalize_preview(s or \"\"), limit=limit)\n",
    "\n",
    "    n_text = len(story_text)\n",
    "\n",
    "    for i, seg in enumerate(extracted_scenes):\n",
    "        segment_id   = seg.get(\"segment_id\", \"unknown\")\n",
    "        segment_type = seg.get(\"segment_type\", \"unknown\")\n",
    "        confidence   = seg.get(\"confidence\", -1.0)\n",
    "        reason       = seg.get(\"reason\", \"unknown\")\n",
    "        summary      = seg.get(\"summary\", \"\")\n",
    "\n",
    "        cohesion     = seg.get(\"cohesion\", {}) or {}\n",
    "        gacd         = seg.get(\"gacd\", None)\n",
    "        erac         = seg.get(\"erac\", None)\n",
    "\n",
    "        # Anchors & paragraph ids (new fields)\n",
    "        start_par_id = seg.get(\"start_par_id\", None)\n",
    "        end_par_id   = seg.get(\"end_par_id\", None)\n",
    "        start_exact  = seg.get(\"start_exact\", \"\") or \"\"\n",
    "        end_exact    = seg.get(\"end_exact\", \"\") or \"\"\n",
    "        start_prefix = seg.get(\"start_prefix\", \"\") or \"\"\n",
    "        end_suffix   = seg.get(\"end_suffix\", \"\") or \"\"\n",
    "\n",
    "        # Offsets (may be missing/invalid)\n",
    "        start_char = seg.get(\"start_char\", None)\n",
    "        end_char   = seg.get(\"end_char\", None)\n",
    "\n",
    "        def _valid_span(a, b):\n",
    "            return isinstance(a, int) and isinstance(b, int) and 0 <= a < b <= n_text\n",
    "\n",
    "        span_note = \"\"\n",
    "        if not _valid_span(start_char, end_char):\n",
    "            # Derive from anchors if possible (raw text; no normalization here)\n",
    "            s_idx = story_text.find(start_exact) if start_exact else -1\n",
    "            if s_idx != -1:\n",
    "                e_pos = story_text.find(end_exact, s_idx) if end_exact else -1\n",
    "                if e_pos != -1:\n",
    "                    start_char = s_idx\n",
    "                    end_char = e_pos + len(end_exact)\n",
    "                    if _valid_span(start_char, end_char):\n",
    "                        span_note = \" (derived from anchors)\"\n",
    "                    else:\n",
    "                        start_char = end_char = None\n",
    "                else:\n",
    "                    # fallback: partial window from start_exact\n",
    "                    if start_exact:\n",
    "                        start_char = s_idx\n",
    "                        end_char = min(n_text, s_idx + max(len(start_exact), 120))\n",
    "                        if _valid_span(start_char, end_char):\n",
    "                            span_note = \" (partial span from start_exact)\"\n",
    "                        else:\n",
    "                            start_char = end_char = None\n",
    "\n",
    "        length_str = (\n",
    "            f\"{end_char - start_char} chars\" if _valid_span(start_char, end_char) else \"unknown\"\n",
    "        )\n",
    "\n",
    "        print(f\"Segment {i}: Type {segment_type} (Confidence: {confidence})\")\n",
    "        print(f\" - Segmentation Rationale: {_sm_norm(reason, 200)}\")\n",
    "        print(f\" - Summary: {_sm_norm(summary, 200)}\")\n",
    "        print(\n",
    "            f\" - Segment ID: {segment_id}, Chars: [{start_char}:{end_char}] {span_note}, Length: {length_str}\"\n",
    "        )\n",
    "\n",
    "        # Paragraph & anchors (normalized+sm for readability)\n",
    "        print(f\" - Paragraphs: start_par_id={start_par_id}, end_par_id={end_par_id}\")\n",
    "        print(\n",
    "            \" - Anchors:\"\n",
    "            f\"\\n     start_prefix='{_sm_norm(start_prefix, 80)}'\"\n",
    "            f\"\\n     start_exact ='{_sm_norm(start_exact, 120)}'\"\n",
    "            f\"\\n     end_exact   ='{_sm_norm(end_exact, 120)}'\"\n",
    "            f\"\\n     end_suffix  ='{_sm_norm(end_suffix, 80)}'\"\n",
    "        )\n",
    "\n",
    "        # Cohesion pretty-print\n",
    "        time_ = cohesion.get(\"time\", \"\")\n",
    "        place = cohesion.get(\"place\", \"\")\n",
    "        chars = cohesion.get(\"characters\", [])\n",
    "        # Normalize the string fields for display\n",
    "        print(f\" - Cohesion: time='{_sm_norm(time_, 120)}', place='{_sm_norm(place, 120)}', characters={chars}\")\n",
    "\n",
    "        if gacd:\n",
    "            # Normalize each field of GACD for display\n",
    "            g_goal = _sm_norm((gacd or {}).get(\"goal\", \"\"), 140)\n",
    "            g_act  = _sm_norm((gacd or {}).get(\"action\", \"\"), 140)\n",
    "            g_con  = _sm_norm((gacd or {}).get(\"conflict\", \"\"), 140)\n",
    "            g_out  = (gacd or {}).get(\"outcome\", \"\")\n",
    "            print(f\" - GACD: goal='{g_goal}', action='{g_act}', conflict='{g_con}', outcome='{g_out}'\")\n",
    "        if erac:\n",
    "            e_emo = _sm_norm((erac or {}).get(\"emotion\", \"\"), 140)\n",
    "            e_rea = _sm_norm((erac or {}).get(\"reason\", \"\"), 140)\n",
    "            e_ant = _sm_norm((erac or {}).get(\"anticipation\", \"\"), 140)\n",
    "            e_cho = _sm_norm((erac or {}).get(\"choice\", \"\"), 140)\n",
    "            print(f\" - ERAC: emotion='{e_emo}', reason='{e_rea}', anticipation='{e_ant}', choice='{e_cho}'\")\n",
    "\n",
    "        # Optional: show a normalized + sm preview slice if we have a valid span\n",
    "        if _valid_span(start_char, end_char):\n",
    "            snippet = story_text[start_char:end_char]\n",
    "            print(f\" - Preview: {_normalize_preview(snippet)[:200]}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "display_revised(example_story.body, revised_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0026d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bdf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33127e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_segments(story_text, extracted_scenes, *, preview_limit=160):\n",
    "    \"\"\"\n",
    "    Validate coverage and overlaps for extracted segments.\n",
    "\n",
    "    Returns:\n",
    "      missing_components: list of dicts for uncovered ranges:\n",
    "        - type: 'start_gap' | 'gap' | 'end_gap'\n",
    "        - start, end, length\n",
    "        - preview\n",
    "        - left_segment_id/right_segment_id (for 'gap')\n",
    "      overlapping_components: list of dicts for overlaps:\n",
    "        - type: 'partial_overlap' | 'contained' | 'duplicate'\n",
    "        - a_index, b_index (indices in start-sorted order)\n",
    "        - a_segment_id, b_segment_id\n",
    "        - start, end, length\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from lcats import utils\n",
    "\n",
    "    n = len(story_text)\n",
    "\n",
    "    def _normalize_preview(s: str) -> str:\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "        s = re.sub(r\"\\n{2,}\", \"\\u2029\", s)  # mark paragraph breaks\n",
    "        s = s.replace(\"\\n\", \" \")            # single newlines -> spaces\n",
    "        s = re.sub(r\"[ \\t\\u00A0]+\", \" \", s).strip()\n",
    "        return s.replace(\"\\u2029\", \"\\n\")    # restore paragraph breaks to single LF\n",
    "\n",
    "    def _sm(text: str, limit: int = preview_limit) -> str:\n",
    "        return utils.sm(_normalize_preview(text or \"\"), limit=limit)\n",
    "\n",
    "    def _valid(a, b) -> bool:\n",
    "        return isinstance(a, int) and isinstance(b, int) and 0 <= a < b <= n\n",
    "\n",
    "    # Gather valid spans\n",
    "    spans = []\n",
    "    for idx, seg in enumerate(extracted_scenes):\n",
    "        s = seg.get(\"start_char\")\n",
    "        e = seg.get(\"end_char\")\n",
    "        if _valid(s, e):\n",
    "            spans.append({\n",
    "                \"i\": idx,\n",
    "                \"segment_id\": seg.get(\"segment_id\", f\"#{idx}\"),\n",
    "                \"start\": int(s),\n",
    "                \"end\": int(e),\n",
    "            })\n",
    "\n",
    "    # Sort by start (then end)\n",
    "    spans.sort(key=lambda d: (d[\"start\"], d[\"end\"]))\n",
    "\n",
    "    missing_components = []\n",
    "    overlapping_components = []\n",
    "\n",
    "    if not spans:\n",
    "        # Nothing valid; entire story is missing if non-empty\n",
    "        if n > 0:\n",
    "            missing_components.append({\n",
    "                \"type\": \"start_gap\",\n",
    "                \"start\": 0, \"end\": n, \"length\": n,\n",
    "                \"preview\": _sm(story_text[0:n]),\n",
    "            })\n",
    "        return missing_components, overlapping_components\n",
    "\n",
    "    # Coverage sweep: track the farthest right endpoint we've covered so far\n",
    "    # as 'covered_end'. Also track last-by-start (prev_seg) for pairwise overlap\n",
    "    # and 'max_seg' as the segment currently contributing the farthest end.\n",
    "    covered_end = 0\n",
    "    prev_seg = None\n",
    "    max_seg = None\n",
    "\n",
    "    for pos, cur in enumerate(spans):\n",
    "        s, e = cur[\"start\"], cur[\"end\"]\n",
    "\n",
    "        # GAP relative to covered_end?\n",
    "        # If the next segment starts after everything we've covered so far,\n",
    "        # the uncovered chunk is [covered_end, s).\n",
    "        if s > covered_end:\n",
    "            gap_type = \"start_gap\" if covered_end == 0 else \"gap\"\n",
    "            gap = {\n",
    "                \"type\": gap_type,\n",
    "                \"start\": covered_end,\n",
    "                \"end\": s,\n",
    "                \"length\": s - covered_end,\n",
    "                \"preview\": _sm(story_text[covered_end:s]),\n",
    "            }\n",
    "            if gap_type == \"gap\" and prev_seg is not None:\n",
    "                gap[\"left_segment_id\"] = prev_seg[\"segment_id\"]\n",
    "                gap[\"right_segment_id\"] = cur[\"segment_id\"]\n",
    "            missing_components.append(gap)\n",
    "\n",
    "        # OVERLAP checks\n",
    "        if prev_seg is not None and s < prev_seg[\"end\"]:\n",
    "            ov_start = max(prev_seg[\"start\"], s)\n",
    "            ov_end = min(prev_seg[\"end\"], e)\n",
    "            otype = \"duplicate\" if (s == prev_seg[\"start\"] and e == prev_seg[\"end\"]) \\\n",
    "                else (\"contained\" if (s >= prev_seg[\"start\"] and e <= prev_seg[\"end\"]) else \"partial_overlap\")\n",
    "            overlapping_components.append({\n",
    "                \"type\": otype,\n",
    "                \"a_index\": pos - 1, \"b_index\": pos,\n",
    "                \"a_segment_id\": prev_seg[\"segment_id\"],\n",
    "                \"b_segment_id\": cur[\"segment_id\"],\n",
    "                \"start\": ov_start, \"end\": ov_end, \"length\": max(0, ov_end - ov_start),\n",
    "            })\n",
    "\n",
    "        # Containment vs the current max-extent segment (if different from prev)\n",
    "        if max_seg is not None and max_seg is not prev_seg:\n",
    "            if s < max_seg[\"end\"]:  # cur overlaps the max-extent segment\n",
    "                if s >= max_seg[\"start\"] and e <= max_seg[\"end\"]:\n",
    "                    # cur fully contained in max_seg\n",
    "                    overlapping_components.append({\n",
    "                        \"type\": \"contained\",\n",
    "                        \"a_index\": max_seg[\"i\"], \"b_index\": cur[\"i\"],\n",
    "                        \"a_segment_id\": max_seg[\"segment_id\"],\n",
    "                        \"b_segment_id\": cur[\"segment_id\"],\n",
    "                        \"start\": s, \"end\": e, \"length\": e - s,\n",
    "                    })\n",
    "                elif e > max_seg[\"end\"]:\n",
    "                    # partial overlap across max_seg right edge\n",
    "                    overlapping_components.append({\n",
    "                        \"type\": \"partial_overlap\",\n",
    "                        \"a_index\": max_seg[\"i\"], \"b_index\": cur[\"i\"],\n",
    "                        \"a_segment_id\": max_seg[\"segment_id\"],\n",
    "                        \"b_segment_id\": cur[\"segment_id\"],\n",
    "                        \"start\": max(s, max_seg[\"start\"]),\n",
    "                        \"end\": max_seg[\"end\"],\n",
    "                        \"length\": max_seg[\"end\"] - max(s, max_seg[\"start\"]),\n",
    "                    })\n",
    "\n",
    "        # Update coverage frontier: farthest end wins\n",
    "        if e > covered_end:\n",
    "            covered_end = e\n",
    "\n",
    "        # Update max-extent segment\n",
    "        if max_seg is None or e > max_seg[\"end\"]:\n",
    "            max_seg = cur\n",
    "\n",
    "        # Advance 'prev-by-start'\n",
    "        prev_seg = cur\n",
    "\n",
    "    # END GAP after the farthest covered end\n",
    "    if covered_end < n:\n",
    "        missing_components.append({\n",
    "            \"type\": \"end_gap\",\n",
    "            \"start\": covered_end, \"end\": n, \"length\": n - covered_end,\n",
    "            \"preview\": _sm(story_text[covered_end:n]),\n",
    "        })\n",
    "\n",
    "    return missing_components, overlapping_components\n",
    "\n",
    "\n",
    "\n",
    "missing_components, overlapping_components = validate_segments(example_story.body, revised_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d395f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
