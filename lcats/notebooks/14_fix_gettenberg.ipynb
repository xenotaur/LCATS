{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc2f052",
   "metadata": {},
   "source": [
    "# Debug Gettenberg Issues\n",
    "\n",
    "Analyze things wrong with the gettenberg API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7474ef",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "import datetime\n",
    "import fnmatch\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c00f7",
   "metadata": {},
   "source": [
    "Third-party modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ee772",
   "metadata": {},
   "source": [
    "Switch to the parent directory so paths can resolve and we write to the right directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e76f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd().resolve()\n",
    "project_root = cwd.parent if cwd.name == \"notebooks\" else cwd\n",
    "scripts_dir = project_root / \"scripts\"\n",
    "if scripts_dir.is_dir():\n",
    "    if cwd != project_root:\n",
    "        print(f\"Changing working directory from {cwd} to {project_root}\")\n",
    "        os.chdir(project_root)  # Change to the project root directory.\n",
    "print(\"Working directory:\", pathlib.Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008fdf8",
   "metadata": {},
   "source": [
    "Add imports from within the project (depends on prior cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837aa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcats import constants\n",
    "from lcats import stories\n",
    "\n",
    "from lcats import utils\n",
    "from lcats.utils import names\n",
    "from lcats.utils import values\n",
    "\n",
    "from lcats.gettenberg import api\n",
    "from lcats.gettenberg import cache\n",
    "from lcats.gettenberg import metadata\n",
    "from lcats.gettenberg import headers\n",
    "\n",
    "from lcats.gatherers import downloaders\n",
    "from lcats.gatherers.mass_quantities import storymap\n",
    "from lcats.gatherers.mass_quantities import parser\n",
    "\n",
    "from lcats.analysis import corpus_surveyor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6ae47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "RELOAD_MODULES = [\n",
    "    api,\n",
    "    cache,\n",
    "    constants,\n",
    "    corpus_surveyor,\n",
    "    downloaders,\n",
    "    headers,\n",
    "    metadata,\n",
    "    names,\n",
    "    parser,\n",
    "    stories,\n",
    "    storymap,\n",
    "    utils,\n",
    "]\n",
    "def reloader():\n",
    "    for module in RELOAD_MODULES:\n",
    "        print(\"Reloading\", module)\n",
    "        reload(module)\n",
    "    print(\"Reloading complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "\n",
    "# See which copy you actually loaded\n",
    "import gutenbergpy, gutenbergpy.parse.rdfparser, gutenbergpy.caches.sqlitecache\n",
    "print(\"gutenbergpy     :\", gutenbergpy.__file__)\n",
    "print(\"rdfparser       :\", gutenbergpy.parse.rdfparser.__file__)\n",
    "print(\"sqlitecache     :\", gutenbergpy.caches.sqlitecache.__file__)\n",
    "print(\"sys.path[0]     :\", sys.path[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f362b7",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f0fcf",
   "metadata": {},
   "source": [
    "### Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the notebook is executing (absolute, resolved)\n",
    "CURRENT_PATH = pathlib.Path.cwd().resolve()\n",
    "\n",
    "# Project root = formerly parent of notebooks/, now just current dir\n",
    "# PROJECT_ROOT = CURRENT_PATH.parent \n",
    "PROJECT_ROOT = CURRENT_PATH\n",
    "\n",
    "# Local data/output inside the project\n",
    "DEV_CORPUS = (PROJECT_ROOT / \"data\")\n",
    "DEV_OUTPUT = (PROJECT_ROOT / \"output\")\n",
    "\n",
    "# Sibling-level resources (one level up from project root)\n",
    "GIT_CORPUS = (PROJECT_ROOT.parent / \"corpora\")\n",
    "OPENIA_API_KEYS_ENV = (PROJECT_ROOT.parent / \".secrets\" / \"openai_api_keys.env\")\n",
    "\n",
    "def check_path(path: pathlib.Path, description: str) -> None:\n",
    "    if path.exists():\n",
    "        print(f\"Found {description} at: {path}\")\n",
    "    else:\n",
    "        print(f\"Missing {description} from: {path}\")\n",
    "\n",
    "check_path(DEV_CORPUS, \"DEV_CORPUS\")\n",
    "check_path(DEV_OUTPUT, \"DEV_OUTPUT\")\n",
    "check_path(GIT_CORPUS, \"GIT_CORPUS\")\n",
    "check_path(OPENIA_API_KEYS_ENV, \"OPENIA_API_KEYS_ENV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working corpora\n",
    "# CORPORA_ROOT = project_root / \"data\"\n",
    "# Checked-in corpora\n",
    "CORPORA_ROOT = project_root / \"..\" / \"corpora\"\n",
    "CORPORA_ROOT = CORPORA_ROOT.resolve()  # Resolve to absolute path.\n",
    "\n",
    "print(\"Corpora root:\", CORPORA_ROOT)\n",
    "print(\"Corpora top-level directories:\", end=\" \")\n",
    "os.listdir(CORPORA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd193904",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stories = corpus_surveyor.find_corpus_stories(CORPORA_ROOT)\n",
    "len(json_stories)\n",
    "print(utils.sml(json_stories))\n",
    "print(\"Type of path element:\", type(json_stories[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e20264",
   "metadata": {},
   "source": [
    "## Gathering Mass Quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4176d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_stories = storymap.SINGLE_STORIES\n",
    "len(single_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c08482",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_stories = single_stories[:10]  # For testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gatherer = downloaders.DataGatherer(\n",
    "    storymap.TARGET_DIRECTORY,\n",
    "    description=\"Single stories from Gutenberg\",\n",
    "    license=\"Public domain, from Project Gutenberg.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = 24927  # Specific story for testing.\n",
    "print(\"Example story:\", example_story)\n",
    "\n",
    "story, filename, error =parser.gather_story(gatherer, example_story)\n",
    "print(\"Filename:\", filename)\n",
    "print(\"Error:\", error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_story = random.choice(single_stories)\n",
    "print(\"Random story:\", random_story)\n",
    "\n",
    "story, filename, error = parser.gather_story(gatherer, random_story)\n",
    "print(\"Filename:\", filename)\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_analyzer(story_id: int) -> None:\n",
    "    print(f\"Analyzing story ID: {story_id}\")\n",
    "\n",
    "    subject = api.get_metadata('subject', story_id)\n",
    "    is_subject_ok = parser.subject_ok(subject)\n",
    "    print(\"Subject:\", subject)\n",
    "    print(\" - Subject OK?\", is_subject_ok)\n",
    "\n",
    "    language = api.get_metadata('language', story_id)\n",
    "    is_language_ok = parser.only_english(language)\n",
    "    print(\"Language:\", language)\n",
    "    print(\" - Is language OK?\", is_language_ok)\n",
    "\n",
    "    title = api.get_metadata('title', story_id)\n",
    "    is_title_ok = parser.title_ok(title)\n",
    "    print(\"Title:\", title)\n",
    "    print(\" - Is title OK?\", is_title_ok)\n",
    "\n",
    "    author = list(api.get_metadata('author', story_id))\n",
    "    is_author_ok = parser.author_ok(author)\n",
    "    print(\"Author:\", author)\n",
    "    print(\" - Is author OK?\", is_author_ok)\n",
    "\n",
    "    print(\"Overall, is story metadata OK?\",\n",
    "          all([is_subject_ok, is_language_ok, is_title_ok, is_author_ok]))\n",
    "\n",
    "    # text = str(headers.strip_headers(api.load_etext(story_id).strip()))\n",
    "    etext = api.load_etext(story_id)\n",
    "    print(etext[:500])\n",
    "    stripped_text = headers.strip_headers(etext.strip()).strip()\n",
    "    print(stripped_text[:500])\n",
    "    cleaned_text = stripped_text.decode('utf-8', errors='ignore').strip()\n",
    "    print(cleaned_text[:500])\n",
    "\n",
    "\n",
    "    text_length = len(cleaned_text)\n",
    "    num_words = len(cleaned_text.split())\n",
    "    print(\"Length of story text (characters):\", text_length)\n",
    "    print(\"Number of words in story text:\", num_words)\n",
    "\n",
    "    # Extract the title and body of the story.\n",
    "    extracted_title = list(title)[0]\n",
    "    print(\"Title extracted:\", extracted_title)\n",
    "    number_of_titles = parser.how_many_titles(cleaned_text, extracted_title)\n",
    "    print(\"Number of titles found in text:\", number_of_titles)\n",
    "\n",
    "    body = parser.body_of_text(cleaned_text, author, extracted_title, True)\n",
    "    print(\"Extracted body length:\", len(body))\n",
    "    print(\"Extracted body preview:\", body[:200])\n",
    "\n",
    "    if len(body) < 10:\n",
    "        print(\"Story is too short, skipping: \" + str(story))\n",
    "\n",
    "    # if we get here, we have the pieces of the story, so let's save\n",
    "    file_name = names.title_to_filename(\n",
    "        extracted_title, ext=constants.FILE_SUFFIX, max_len=50)\n",
    "    print(\"Generated filename:\", file_name)\n",
    "    \n",
    "\n",
    "story_analyzer(example_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_analyzer(23920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = api.get_metadata('title', example_story)\n",
    "is_title_ok = parser.title_ok(title)\n",
    "print(\"Title:\", title)\n",
    "print(\" - Is title OK?\", is_title_ok)\n",
    "\n",
    "author = list(api.get_metadata('author', example_story))\n",
    "is_author_ok = parser.author_ok(author)\n",
    "print(\"Author:\", author)\n",
    "print(\" - Is author OK?\", is_author_ok)\n",
    "\n",
    "etext = api.load_etext(example_story)\n",
    "print(etext[:500])\n",
    "cleaned_text = etext.decode('utf-8', errors='ignore').strip()\n",
    "print(cleaned_text[:500])\n",
    "\n",
    "# cleaned_text = str(headers.strip_headers(api.load_etext(example_story).strip()))\n",
    "text_length = len(cleaned_text)\n",
    "num_words = len(cleaned_text.split())\n",
    "print(\"Length of story text (characters):\", text_length)\n",
    "print(\"Number of words in story text:\", num_words)\n",
    "\n",
    "# Extract the title and body of the story.\n",
    "extracted_title = list(title)[0]\n",
    "print(\"Title extracted:\", extracted_title)\n",
    "number_of_titles = parser.how_many_titles(cleaned_text, extracted_title)\n",
    "print(\"Number of titles found in text:\", number_of_titles)\n",
    "\n",
    "paragraph_array = cleaned_text.split(\"\\n\\n\")\n",
    "print(\"Number of paragraphs found in text:\", len(paragraph_array))\n",
    "\n",
    "body = parser.body_of_text(cleaned_text, author, extracted_title, True)\n",
    "print(\"Extracted body length:\", len(body))\n",
    "print(\"Extracted body preview:\", body[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_story = random.choice(single_stories)\n",
    "print(\"Random story ID:\", random_story)\n",
    "\n",
    "api.get_metadata('subject', random_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text.split('\\n')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e50db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6838eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_stories = corpus_surveyor.find_corpus_stories(DEV_CORPUS)\n",
    "len(dev_stories)\n",
    "print(utils.sml(dev_stories))\n",
    "print(\"Type of path element:\", type(dev_stories[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def titles_for(cache, book_id: int) -> Set[str]:\n",
    "    \"\"\"Return set of title strings for a given Gutenberg book ID.\"\"\"\n",
    "    rows = cache.native_query(\n",
    "        f\"\"\"\n",
    "        SELECT t.name AS v\n",
    "        FROM titles t\n",
    "        JOIN books b ON t.bookid = b.id\n",
    "        WHERE b.gutenbergbookid = {int(book_id)}\n",
    "        \"\"\"\n",
    "    )\n",
    "    return values.strings_from_sql(rows)\n",
    "\n",
    "def text_for(book_id: int):\n",
    "    etext = api.load_etext(book_id)\n",
    "    cleaned_text = etext.decode('utf-8', errors='ignore').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def summary_of(book_id):\n",
    "    gut_cache = cache.ensure_gutenberg_cache()\n",
    "    book_titles = titles_for(gut_cache, book_id)\n",
    "    book_text = text_for(book_id)\n",
    "    print(\"Book ID: \", book_id)\n",
    "    print(\" - Book Titles:\", book_titles)\n",
    "    print(\" - Book Text:\", book_text[:500])\n",
    "    return book_text, book_titles\n",
    "\n",
    "\n",
    "text, titles = summary_of(example_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4636a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_story = random.choice(single_stories)\n",
    "random_text, random_titles = summary_of(random_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcats.gettenberg import cache\n",
    "\n",
    "print(\"DB path:\", cache.gutenberg_cache_path())\n",
    "print(\"DB size:\", cache.gutenberg_cache_path().stat().st_size if cache.gutenberg_cache_path().exists() else 0)\n",
    "print(\"Texts dir:\", cache.GUTENBERG_TEXTS.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1241afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.gutenberg_cache_path().stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, pathlib\n",
    "db = pathlib.Path(cache.gutenberg_cache_path())\n",
    "con = sqlite3.connect(f\"file:{db}?mode=ro\", uri=True)\n",
    "\n",
    "print(\"titles:\", list(con.execute(\"PRAGMA table_info(titles);\")))\n",
    "print(\"books :\", list(con.execute(\"PRAGMA table_info(books);\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ffdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gid = 3115\n",
    "\n",
    "# What is the internal PK for this Gutenberg ID?\n",
    "[(book_pk,)] = list(con.execute(f\"SELECT id FROM books WHERE gutenbergbookid={gid}\"))\n",
    "print(\"Internal PK for Gutenberg ID\", gid, \"is\", book_pk)\n",
    "\n",
    "# What title rows link to that PK?\n",
    "print(\"Titles for book PK\", book_pk, \":\")\n",
    "print(list(con.execute(f\"SELECT name FROM titles WHERE bookid={book_pk}\")))\n",
    "# Cross-check via join (should match):\n",
    "print(\"Titles via join:\")\n",
    "print(list(con.execute(f\"\"\"\n",
    "    SELECT t.name\n",
    "    FROM titles t JOIN books b ON t.bookid=b.id\n",
    "    WHERE b.gutenbergbookid={gid}\n",
    "\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ced463",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, titles = summary_of(3115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcats.gettenberg import cache\n",
    "from gutenbergpy import gutenbergcache as gc\n",
    "import pathlib, sqlite3\n",
    "\n",
    "print(\"Settings:\")\n",
    "print(\"  CACHE_FILENAME:\",           gc.GutenbergCacheSettings.CACHE_FILENAME)\n",
    "print(\"  CACHE_RDF_ARCHIVE_NAME:\",   gc.GutenbergCacheSettings.CACHE_RDF_ARCHIVE_NAME)\n",
    "print(\"  CACHE_RDF_UNPACK_DIRECTORY:\", gc.GutenbergCacheSettings.CACHE_RDF_UNPACK_DIRECTORY)\n",
    "print(\"  TEXT_FILES_CACHE_FOLDER:\",  gc.GutenbergCacheSettings.TEXT_FILES_CACHE_FOLDER)\n",
    "\n",
    "db = pathlib.Path(gc.GutenbergCacheSettings.CACHE_FILENAME)\n",
    "print(\"\\nDB path exists/size:\", db, db.exists(), (db.stat().st_size if db.exists() else 0))\n",
    "\n",
    "# Verify the schema columns you actually have\n",
    "con = sqlite3.connect(f\"file:{db}?mode=ro\", uri=True)\n",
    "print(\"PRAGMA titles:\", list(con.execute(\"PRAGMA table_info(titles)\")))\n",
    "print(\"PRAGMA books :\", list(con.execute(\"PRAGMA table_info(books)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gid = 24927\n",
    "# internal PK for this Gutenberg ID\n",
    "row = list(con.execute(\"SELECT id FROM books WHERE gutenbergbookid=?\", (gid,)))\n",
    "print(\"book PK:\", row)\n",
    "if row:\n",
    "    (book_pk,) = row[0]\n",
    "    print(\"titles for PK:\", list(con.execute(\"SELECT name FROM titles WHERE bookid=?\", (book_pk,))))\n",
    "    print(\"join check:\", list(con.execute(\"\"\"\n",
    "        SELECT t.name\n",
    "        FROM titles t JOIN books b ON t.bookid=b.id\n",
    "        WHERE b.gutenbergbookid=?\"\"\", (gid,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "unpack = Path(gc.GutenbergCacheSettings.CACHE_RDF_UNPACK_DIRECTORY)\n",
    "rdf = unpack / str(gid) / f\"pg{gid}.rdf\"\n",
    "print(\"RDF exists:\", rdf, rdf.exists(), rdf.stat().st_size if rdf.exists() else 0)\n",
    "if rdf.exists():\n",
    "    print(\"first 200 bytes:\", rdf.read_bytes()[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "gid = 24927\n",
    "\n",
    "# A) Does titles(bookid) already store the Gutenberg ID?\n",
    "print(list(con.execute(\"SELECT name FROM titles WHERE bookid=?\", (gid,))))\n",
    "\n",
    "# B) What do we get if we join titles.bookid to books.gutenbergbookid?\n",
    "print(list(con.execute(\"\"\"\n",
    "    SELECT t.name\n",
    "    FROM titles t\n",
    "    JOIN books b ON t.bookid = b.gutenbergbookid\n",
    "    WHERE b.gutenbergbookid=?\"\"\", (gid,))))\n",
    "\n",
    "# C) Your current join (bookid -> books.id), which yields the *wrong* title:\n",
    "print(list(con.execute(\"\"\"\n",
    "    SELECT t.name\n",
    "    FROM titles t\n",
    "    JOIN books b ON t.bookid = b.id\n",
    "    WHERE b.gutenbergbookid=?\"\"\", (gid,))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from lcats.gettenberg import api, headers\n",
    "\n",
    "def header_titles_for(gid: int) -> set[str]:\n",
    "    txt = api.load_etext(gid)\n",
    "    hdr = headers.get_text_header_lines(txt)\n",
    "    return { line.split(\":\",1)[1].strip()\n",
    "             for line in hdr if line.lower().startswith(\"title:\") }\n",
    "\n",
    "gids = [r[0] for r in con.execute(\"SELECT gutenbergbookid FROM books ORDER BY RANDOM() LIMIT 50\")]\n",
    "\n",
    "mismatch_id_join = 0\n",
    "mismatch_gid_join = 0\n",
    "\n",
    "for gid in gids:\n",
    "    t_id_join  = {r[0] for r in con.execute(\n",
    "        \"\"\"SELECT t.name FROM titles t\n",
    "           JOIN books b ON t.bookid=b.id\n",
    "           WHERE b.gutenbergbookid=?\"\"\", (gid,))}\n",
    "    t_gid_join = {r[0] for r in con.execute(\n",
    "        \"\"\"SELECT t.name FROM titles t\n",
    "           JOIN books b ON t.bookid=b.gutenbergbookid\n",
    "           WHERE b.gutenbergbookid=?\"\"\", (gid,))}\n",
    "    try:\n",
    "        t_header   = header_titles_for(gid)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting header titles for {gid}: {e}\")\n",
    "        t_header = set()\n",
    "\n",
    "    if t_header and t_id_join and t_header.isdisjoint(t_id_join):\n",
    "        mismatch_id_join += 1\n",
    "    if t_header and t_gid_join and t_header.isdisjoint(t_gid_join):\n",
    "        mismatch_gid_join += 1\n",
    "\n",
    "print(\"mismatch w/ bookid→books.id join:\", mismatch_id_join)\n",
    "print(\"mismatch w/ bookid→books.gutenbergbookid join:\", mismatch_gid_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_cache = cache.ensure_gutenberg_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db485fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) What title does your DB think belongs to 24927?\n",
    "list(gutenberg_cache.native_query(\"\"\"\n",
    "SELECT t.name\n",
    "FROM titles t\n",
    "JOIN books b ON t.bookid=b.id\n",
    "WHERE b.gutenbergbookid=24927\n",
    "\"\"\"))\n",
    "# -> [('The Red Cross Girls with Pershing to Victory',)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac802929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Which Gutenberg ID does that wrong title actually belong to?\n",
    "list(gutenberg_cache.native_query(\"\"\"\n",
    "SELECT b.gutenbergbookid\n",
    "FROM titles t\n",
    "JOIN books b ON t.bookid=b.id\n",
    "WHERE t.name='The Red Cross Girls with Pershing to Victory'\n",
    "\"\"\"))\n",
    "# Expect a different gid (not 24927). If you see a plausible but wrong gid,\n",
    "# we have clear evidence of mis-association during cache creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6873fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Sanity: count how widespread this is on a sample\n",
    "import random\n",
    "sample = [row[0] for row in gutenberg_cache.native_query(\"SELECT gutenbergbookid FROM books LIMIT 500\")]\n",
    "def db_title(gid):\n",
    "    return next(iter(gutenberg_cache.native_query(f\"\"\"\n",
    "        SELECT t.name FROM titles t JOIN books b ON t.bookid=b.id\n",
    "        WHERE b.gutenbergbookid={gid}\"\"\")), (None,))[0]\n",
    "\n",
    "def header_title(gid):\n",
    "    raw = api.load_etext(gid)\n",
    "    for line in headers.get_text_header_lines(raw):\n",
    "        if line.lower().startswith(\"title:\"):\n",
    "            return line.split(\":\",1)[1].strip()\n",
    "    return None\n",
    "\n",
    "mismatches = [(gid, db_title(gid), header_title(gid)) for gid in random.sample(sample, 50)]\n",
    "[m for m in mismatches if m[1] and m[2] and m[1] != m[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207dd8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_mismatch = [m for m in mismatches if m[1] and m[2] and not m[1].lower().startswith(m[2].lower())]\n",
    "significant_mismatch, len(significant_mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc97971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Find internal PK for the Gutenberg ID\n",
    "pk = list(gutenberg_cache.native_query(\"SELECT id FROM books WHERE gutenbergbookid=24927\"))[0][0]\n",
    "\n",
    "# 2) Titles attached to that PK:\n",
    "list(gutenberg_cache.native_query(f\"SELECT name FROM titles WHERE bookid={pk}\"))\n",
    "\n",
    "# 3) Cross-check: the one line join you’re using\n",
    "list(gutenberg_cache.native_query(\"\"\"\n",
    "  SELECT t.name\n",
    "  FROM titles t JOIN books b ON t.bookid=b.id\n",
    "  WHERE b.gutenbergbookid=24927\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324df7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect, gutenbergpy.parse.parseitemtitles as pit\n",
    "import gutenbergpy.parse.rdfparser as rp\n",
    "import gutenbergpy.caches.sqlitecache as sc\n",
    "\n",
    "print(inspect.getsource(pit))  # how titles are extracted/applied\n",
    "print(inspect.getsource(rp))   # how book key is computed and passed around\n",
    "print(inspect.getsource(sc))   # where INSERTs into titles happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d62631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "# library imports\n",
    "from gutenbergpy import gutenbergcache as gc\n",
    "from gutenbergpy.parse.rdfparser import RdfParser\n",
    "\n",
    "def run_rdfparser_on_single_rdf(rdf_file: Path):\n",
    "    \"\"\"Run gutenbergpy's RDF parser on one RDF file and return the parser object.\"\"\"\n",
    "    rdf_file = Path(rdf_file)\n",
    "    assert rdf_file.name.startswith(\"pg\") and rdf_file.suffix == \".rdf\", \\\n",
    "        \"Expect a filename like pg<gid>.rdf\"\n",
    "\n",
    "    gid = int(rdf_file.stem[2:])\n",
    "\n",
    "    # Build the unpack tree it expects: <unpack_dir>/<gid>/pg<gid>.rdf\n",
    "    tmp = Path(tempfile.mkdtemp(prefix=\"gpy_rdf_\"))\n",
    "    unpack_dir = tmp / \"epub\" / str(gid)\n",
    "    unpack_dir.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(rdf_file, unpack_dir / rdf_file.name)\n",
    "\n",
    "    # Point ONLY the unpack dir at our temp; no DB/file writes are triggered by RdfParser itself.\n",
    "    S = gc.GutenbergCacheSettings\n",
    "    old_unpack = S.CACHE_RDF_UNPACK_DIRECTORY\n",
    "    try:\n",
    "        gc.GutenbergCacheSettings.set(CacheUnpackDir=str(tmp / \"epub\"))\n",
    "\n",
    "        # Run the parser – this should scan just our single file tree\n",
    "        parser = RdfParser()\n",
    "        result = parser.do()\n",
    "\n",
    "        # `result` is a list/array of per-book field sets the cache layer would normally persist.\n",
    "        return parser, result, tmp\n",
    "    finally:\n",
    "        # restore to avoid surprising other code\n",
    "        gc.GutenbergCacheSettings.set(CacheUnpackDir=old_unpack)\n",
    "\n",
    "# ---- usage ----\n",
    "# point to a *known good* RDF you already have on disk\n",
    "rdf = Path(\"cache/epub/24927/pg24927.rdf\")\n",
    "parser, parsed, tmp_root = run_rdfparser_on_single_rdf(rdf)\n",
    "\n",
    "print()\n",
    "print(\"Parsed items:\", parsed)\n",
    "# Peek at what the parser produced (shape is implementation-specific; print to inspect)\n",
    "print(\"Parsed book title id:\", parsed.books[0].titles_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43395508",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.books[0].titles_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b74722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from unittest import mock\n",
    "from pprint import pprint\n",
    "\n",
    "from gutenbergpy import gutenbergcache as gc\n",
    "from gutenbergpy.parse.rdfparser import RdfParser\n",
    "import gutenbergpy.caches.sqlitecache as sqlitecache  # the DB writer used by the parser\n",
    "\n",
    "def capture_parser_writes_for_single_rdf(rdf_file: Path):\n",
    "    \"\"\"Run RdfParser on one RDF and capture what it tries to write to the cache.\"\"\"\n",
    "    rdf_file = Path(rdf_file)\n",
    "    gid = int(rdf_file.stem[2:])\n",
    "\n",
    "    tmp = Path(tempfile.mkdtemp(prefix=\"gpy_cap_\"))\n",
    "    unpack_dir = tmp / \"epub\" / str(gid)\n",
    "    unpack_dir.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(rdf_file, unpack_dir / rdf_file.name)\n",
    "\n",
    "    S = gc.GutenbergCacheSettings\n",
    "    old_unpack = S.CACHE_RDF_UNPACK_DIRECTORY\n",
    "\n",
    "    captured = {\n",
    "        \"books\": [],\n",
    "        \"titles\": [],\n",
    "        \"authors\": [],\n",
    "        \"languages\": [],\n",
    "        \"subjects\": [],\n",
    "        # add more buckets if you want to observe other tables\n",
    "    }\n",
    "\n",
    "    # Discover insert-like methods to patch (names can vary a bit between versions)\n",
    "    # Inspect the module or print(dir(sqlitecache.SqliteCache)) if unsure.\n",
    "    # Common names: insert_books, insert_titles, insert_authors, insert_languages, insert_subjects\n",
    "    def cap(name):\n",
    "        def _cap(self, rows):\n",
    "            captured[name].extend(list(rows))\n",
    "        return _cap\n",
    "\n",
    "    with \\\n",
    "        mock.patch.object(sqlitecache.SQLiteCache, \"insert_books\", side_effect=cap(\"books\")),\\\n",
    "        mock.patch.object(sqlitecache.SQLiteCache, \"insert_titles\", side_effect=cap(\"titles\")),\\\n",
    "        mock.patch.object(sqlitecache.SQLiteCache, \"insert_authors\", side_effect=cap(\"authors\"), create=True),\\\n",
    "        mock.patch.object(sqlitecache.SQLiteCache, \"insert_languages\", side_effect=cap(\"languages\"), create=True),\\\n",
    "        mock.patch.object(sqlitecache.SQLiteCache, \"insert_subjects\", side_effect=cap(\"subjects\"), create=True):\n",
    "\n",
    "        try:\n",
    "            gc.GutenbergCacheSettings.set(CacheUnpackDir=str(tmp / \"epub\"))\n",
    "\n",
    "            parser = RdfParser()\n",
    "            parser.do()  # will call the patched insert_* methods instead of touching a DB\n",
    "        finally:\n",
    "            gc.GutenbergCacheSettings.set(CacheUnpackDir=old_unpack)\n",
    "\n",
    "    return captured, tmp\n",
    "\n",
    "# ---- usage ----\n",
    "captured, tmp_root = capture_parser_writes_for_single_rdf(Path(\"cache/epub/24927/pg24927.rdf\"))\n",
    "\n",
    "print(\"BOOK rows:\", captured[\"books\"])\n",
    "print(\"TITLE rows:\", captured[\"titles\"])\n",
    "pprint(captured[\"titles\"][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd47d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
