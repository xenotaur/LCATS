{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc2f052",
   "metadata": {},
   "source": [
    "# Clean the Corpus\n",
    "\n",
    "This notebook analyzes a corpus for files with the wrong filename type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7474ef",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "import datetime\n",
    "import fnmatch\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c00f7",
   "metadata": {},
   "source": [
    "Third-party modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ee772",
   "metadata": {},
   "source": [
    "Switch to the parent directory so paths can resolve and we write to the right directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e76f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd().resolve()\n",
    "project_root = cwd.parent if cwd.name == \"notebooks\" else cwd\n",
    "scripts_dir = project_root / \"scripts\"\n",
    "if scripts_dir.is_dir():\n",
    "    if cwd != project_root:\n",
    "        print(f\"Changing working directory from {cwd} to {project_root}\")\n",
    "        os.chdir(project_root)  # Change to the project root directory.\n",
    "print(\"Working directory:\", pathlib.Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008fdf8",
   "metadata": {},
   "source": [
    "Add imports from within the project (depends on prior cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837aa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcats import constants\n",
    "from lcats import stories\n",
    "from lcats import utils\n",
    "\n",
    "from lcats.analysis import corpus_surveyor\n",
    "from lcats.analysis import graph_plotters\n",
    "from lcats.analysis import llm_extractor\n",
    "from lcats.analysis import scene_analysis\n",
    "from lcats.analysis import story_analysis\n",
    "from lcats.analysis import story_processors\n",
    "from lcats.analysis import text_segmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "RELOAD_MODULES = [\n",
    "    constants,\n",
    "    stories,\n",
    "    corpus_surveyor,\n",
    "    graph_plotters,\n",
    "    llm_extractor,\n",
    "    scene_analysis,\n",
    "    story_analysis,\n",
    "    story_processors,\n",
    "    text_segmenter,\n",
    "    utils,\n",
    "]\n",
    "def reloader():\n",
    "    for module in RELOAD_MODULES:\n",
    "        print(\"Reloading\", module)\n",
    "        reload(module)\n",
    "    print(\"Reloading complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f362b7",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f0fcf",
   "metadata": {},
   "source": [
    "### Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the notebook is executing (absolute, resolved)\n",
    "CURRENT_PATH = pathlib.Path.cwd().resolve()\n",
    "\n",
    "# Project root = formerly parent of notebooks/, now just current dir\n",
    "# PROJECT_ROOT = CURRENT_PATH.parent \n",
    "PROJECT_ROOT = CURRENT_PATH\n",
    "\n",
    "# Local data/output inside the project\n",
    "DEV_CORPUS = (PROJECT_ROOT / \"data\")\n",
    "DEV_OUTPUT = (PROJECT_ROOT / \"output\")\n",
    "\n",
    "# Sibling-level resources (one level up from project root)\n",
    "GIT_CORPUS = (PROJECT_ROOT.parent / \"corpora\")\n",
    "OPENIA_API_KEYS_ENV = (PROJECT_ROOT.parent / \".secrets\" / \"openai_api_keys.env\")\n",
    "\n",
    "def check_path(path: pathlib.Path, description: str) -> None:\n",
    "    if path.exists():\n",
    "        print(f\"Found {description} at: {path}\")\n",
    "    else:\n",
    "        print(f\"Missing {description} from: {path}\")\n",
    "\n",
    "check_path(DEV_CORPUS, \"DEV_CORPUS\")\n",
    "check_path(DEV_OUTPUT, \"DEV_OUTPUT\")\n",
    "check_path(GIT_CORPUS, \"GIT_CORPUS\")\n",
    "check_path(OPENIA_API_KEYS_ENV, \"OPENIA_API_KEYS_ENV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working corpora\n",
    "# CORPORA_ROOT = project_root / \"data\"\n",
    "# Checked-in corpora\n",
    "CORPORA_ROOT = project_root / \"..\" / \"corpora\"\n",
    "CORPORA_ROOT = CORPORA_ROOT.resolve()  # Resolve to absolute path.\n",
    "\n",
    "print(\"Corpora root:\", CORPORA_ROOT)\n",
    "print(\"Corpora top-level directories:\", end=\" \")\n",
    "os.listdir(CORPORA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082a2e4",
   "metadata": {},
   "source": [
    "### OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86951cd",
   "metadata": {},
   "source": [
    "Get the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(OPENIA_API_KEYS_ENV)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f16ea6",
   "metadata": {},
   "source": [
    "Verify that we can get a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b82f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "print(f\"Loaded OpenAI client: {client} with version: {client._version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffb37e",
   "metadata": {},
   "source": [
    "Verify the API is working. This week. And that you have credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ae625",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a starship captain visiting a planet.\"\n",
    ")\n",
    "\n",
    "print(f\"Story generated on: {datetime.date.today()}:\")\n",
    "utils.pprint(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bda9f",
   "metadata": {},
   "source": [
    "## Corpora-level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec8ef1",
   "metadata": {},
   "source": [
    "### Story Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd34286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run from within a notebook, the corpora root is two paths up from the notebook's location.\n",
    "CORPORA_ROOT = GIT_CORPUS  # Checked-in corpora\n",
    "# CORPORA_ROOT = DEV_CORPUS  # Command line working corpora\n",
    "\n",
    "# Now load the corpora\n",
    "corpora = stories.Corpora(CORPORA_ROOT)\n",
    "\n",
    "print(\"Loaded corpora:\")\n",
    "print(f\" - root: {corpora.corpora_root}\")\n",
    "print(f\" - corpora: {len(corpora.corpora)}\")\n",
    "print(f\" - stories: {len(corpora.stories)}\")\n",
    "print()\n",
    "\n",
    "print(f\"Example story: corpora.stories[0]:\")\n",
    "example_story = corpora.stories[0]\n",
    "print(f\"Story type: {type(example_story)} with a body of {len(example_story.body)} characters.\")\n",
    "print(example_story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24e387",
   "metadata": {},
   "source": [
    "### JSON Corpora Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stories = corpus_surveyor.find_corpus_stories(CORPORA_ROOT)\n",
    "len(json_stories)\n",
    "print(utils.sml(json_stories))\n",
    "print(\"Type of path element:\", type(json_stories[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b4489",
   "metadata": {},
   "source": [
    "## Corpora Cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e29c5",
   "metadata": {},
   "source": [
    "The following code should not be run automatically as it processes a whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"The following code should not be run automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VALID_BASENAME_RE_TEMPLATE = r\"^[a-z0-9]+(?:_[a-z0-9]+)*$\"\n",
    "\n",
    "@dataclass\n",
    "class FileRecord:\n",
    "    rel_dir: str\n",
    "    old_basename: str\n",
    "    old_ext: str\n",
    "    new_basename: Optional[str] = None  # set if repaired\n",
    "    new_ext: Optional[str] = None       # defaults to lower of old_ext\n",
    "    reason: Optional[str] = None        # if invalid or changed\n",
    "\n",
    "@dataclass\n",
    "class OperationLog:\n",
    "    params: dict\n",
    "    candidates: List[str]              # relative paths\n",
    "    excluded: List[str]                # relative paths\n",
    "    valid: List[str]                   # relative paths (kept as-is)\n",
    "    invalid: List[str]                 # relative paths (need repair)\n",
    "    repairs: List[dict]                # dict per repaired file (old -> new)\n",
    "    errors: List[Tuple[str, Optional[str], str]]  # (orig_rel, rename_rel|None, reason)\n",
    "    performed_writes: List[str]        # relative paths written (dry_run=False)\n",
    "    performed_skips: List[str]         # relative paths skipped\n",
    "    # Note: path to CHANGES.tab is stored in params[\"changes_tsv\"] if written.\n",
    "\n",
    "def rename_and_fix_json_files(\n",
    "    input_dir: str | pathlib.Path,\n",
    "    output_dir: str | pathlib.Path,\n",
    "    *,\n",
    "    ext: str = \".json\",                      # file extension to process (case-insensitive)\n",
    "    dry_run: bool = True,                    # list-only by default\n",
    "    max_basename_len: int = 72,              # max length excluding extension\n",
    "    exclude_basenames: Optional[List[str]] = None,   # exact basenames to exclude (no dir)\n",
    "    exclude_globs: Optional[List[str]] = None,       # globs against basename (e.g. \"*.md\")\n",
    "    exclude_path_prefixes: Optional[List[str]] = None,  # path prefixes (posix, relative to input)\n",
    "    metadata_name_includes_extension: bool = False,  # whether metadata[\"name\"] includes extension\n",
    "    copy_valid_when_writing: bool = True,    # copy valid files into output when dry_run=False\n",
    ") -> OperationLog:\n",
    "    \"\"\"\n",
    "    Scan input_dir recursively, select files with the given extension (case-insensitive),\n",
    "    exclude according to rules, validate basenames, attempt repairs, detect collisions,\n",
    "    and optionally write fixed JSON files to output_dir (mirroring subtree) while\n",
    "    updating data[\"metadata\"][\"name\"] for repaired files.\n",
    "\n",
    "    Also writes CHANGES.tab (TSV) in output_dir when dry_run=False, listing all planned repairs:\n",
    "      columns: from_rel  to_rel  old_basename  new_basename  old_ext  new_ext\n",
    "\n",
    "    Acceptable filename rules (basename only, no extension):\n",
    "      - lowercase letters and digits only, separated by single underscores\n",
    "      - no leading or trailing underscores\n",
    "      - length <= max_basename_len\n",
    "    \"\"\"\n",
    "    input_dir = pathlib.Path(input_dir).resolve()\n",
    "    output_dir = pathlib.Path(output_dir).resolve()\n",
    "    params_base = {\n",
    "        \"input_dir\": str(input_dir),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"ext\": ext,\n",
    "        \"dry_run\": dry_run,\n",
    "        \"max_basename_len\": max_basename_len,\n",
    "        \"exclude_basenames\": list({*(exclude_basenames or []), \"LICENSE\"}),\n",
    "        \"exclude_globs\": list({*(exclude_globs or []), \"*.md\"}),\n",
    "        \"exclude_path_prefixes\": [p.rstrip(\"/\") + \"/\" for p in (exclude_path_prefixes or [\"cache\"])],\n",
    "        \"metadata_name_includes_extension\": metadata_name_includes_extension,\n",
    "        \"copy_valid_when_writing\": copy_valid_when_writing,\n",
    "    }\n",
    "\n",
    "    if not input_dir.exists():\n",
    "        raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "    exclude_basenames = params_base[\"exclude_basenames\"]\n",
    "    exclude_globs = params_base[\"exclude_globs\"]\n",
    "    exclude_path_prefixes = params_base[\"exclude_path_prefixes\"]\n",
    "\n",
    "    valid_basename_re = re.compile(_VALID_BASENAME_RE_TEMPLATE)\n",
    "\n",
    "    def is_candidate(p: pathlib.Path) -> bool:\n",
    "        return p.is_file() and p.suffix.lower() == ext.lower()\n",
    "\n",
    "    def is_excluded(rel_path: pathlib.Path) -> bool:\n",
    "        rel_posix = rel_path.as_posix()\n",
    "        for pref in exclude_path_prefixes:\n",
    "            if rel_posix.startswith(pref) or rel_posix.startswith(\"./\" + pref):\n",
    "                return True\n",
    "        if rel_path.name in exclude_basenames:\n",
    "            return True\n",
    "        name_lower = rel_path.name.lower()\n",
    "        for g in exclude_globs:\n",
    "            if fnmatch.fnmatch(name_lower, g.lower()):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_valid_basename(basename: str) -> bool:\n",
    "        return (\n",
    "            len(basename) > 0\n",
    "            and len(basename) <= max_basename_len\n",
    "            and bool(valid_basename_re.match(basename))\n",
    "        )\n",
    "\n",
    "    def repair_basename(raw: str) -> str:\n",
    "        \"\"\"\n",
    "        Lowercase, non [a-z0-9] -> '_', collapse runs, strip edge underscores, enforce max length.\n",
    "        \"\"\"\n",
    "        s = raw.lower()\n",
    "        s = re.sub(r\"[^a-z0-9]+\", \"_\", s)  # collapse specials to single underscores\n",
    "        s = s.strip(\"_\")                   # no leading/trailing underscores\n",
    "        if len(s) > max_basename_len:\n",
    "            s = s[:max_basename_len]\n",
    "            s = s.strip(\"_\")               # ensure no edge underscores after truncation\n",
    "        s = re.sub(r\"_+\", \"_\", s)          # re-collapse just in case\n",
    "        return s\n",
    "\n",
    "    # Walk and collect candidates\n",
    "    candidates: List[str] = []\n",
    "    excluded: List[str] = []\n",
    "    valid: List[str] = []\n",
    "    invalid: List[str] = []\n",
    "    records_by_rel: Dict[str, FileRecord] = {}\n",
    "\n",
    "    for p in input_dir.rglob(\"*\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        if not is_candidate(p):\n",
    "            continue\n",
    "        rel = p.relative_to(input_dir)\n",
    "        rel_str = rel.as_posix()\n",
    "        if is_excluded(rel):\n",
    "            excluded.append(rel_str)\n",
    "            continue\n",
    "        candidates.append(rel_str)\n",
    "        base = p.stem\n",
    "        ext_actual = p.suffix  # keep original extension (may be .JSON)\n",
    "        rec = FileRecord(rel_dir=rel.parent.as_posix(), old_basename=base, old_ext=ext_actual)\n",
    "        # valid only if basename meets rule and extension already lowercase\n",
    "        if is_valid_basename(base) and ext_actual == ext_actual.lower():\n",
    "            valid.append(rel_str)\n",
    "        else:\n",
    "            invalid.append(rel_str)\n",
    "            rec.reason = \"invalid\"\n",
    "        records_by_rel[rel_str] = rec\n",
    "\n",
    "    print(f\"[scan] candidates: {len(candidates)}, excluded: {len(excluded)}, \"\n",
    "          f\"valid: {len(valid)}, invalid: {len(invalid)}\")\n",
    "\n",
    "    # Plan repairs for invalid files; detect collisions\n",
    "    errors: List[Tuple[str, Optional[str], str]] = []\n",
    "    planned_new_paths: Dict[str, str] = {}  # rel_new -> rel_old\n",
    "    repairs: List[dict] = []\n",
    "    valid_set = set(valid)\n",
    "\n",
    "    for rel_str in invalid:\n",
    "        rec = records_by_rel[rel_str]\n",
    "        new_base = repair_basename(rec.old_basename)\n",
    "        new_ext = rec.old_ext.lower()  # normalize extension to lowercase\n",
    "\n",
    "        if not new_base:\n",
    "            errors.append((rel_str, None, \"blank_after_repair\"))\n",
    "            continue\n",
    "\n",
    "        rel_dir = rec.rel_dir\n",
    "        new_rel = f\"{rel_dir}/{new_base}{new_ext}\" if rel_dir not in (\"\", \".\") else f\"{new_base}{new_ext}\"\n",
    "\n",
    "        # Collision with existing valid?\n",
    "        if new_rel in valid_set and new_rel != rel_str:\n",
    "            errors.append((rel_str, new_rel, \"collision_with_existing_valid\"))\n",
    "            continue\n",
    "\n",
    "        # Collision with another repair?\n",
    "        if new_rel in planned_new_paths and planned_new_paths[new_rel] != rel_str:\n",
    "            errors.append((rel_str, new_rel, \"collision_with_another_repair\"))\n",
    "            continue\n",
    "\n",
    "        rec.new_basename = new_base\n",
    "        rec.new_ext = new_ext\n",
    "        planned_new_paths[new_rel] = rel_str\n",
    "        repairs.append({\n",
    "            \"from\": rel_str,\n",
    "            \"to\": new_rel,\n",
    "            \"old_basename\": rec.old_basename,\n",
    "            \"new_basename\": new_base,\n",
    "            \"old_ext\": rec.old_ext,\n",
    "            \"new_ext\": new_ext\n",
    "        })\n",
    "\n",
    "    # Abort on errors\n",
    "    if errors:\n",
    "        print(f\"[abort] {len(errors)} error(s) detected; no files written.\")\n",
    "        for orig, newp, reason in errors:\n",
    "            print(f\"  - {reason}: {orig}\" + (f\" -> {newp}\" if newp else \"\"))\n",
    "        params = dict(params_base)\n",
    "        return OperationLog(\n",
    "            params=params,\n",
    "            candidates=candidates,\n",
    "            excluded=excluded,\n",
    "            valid=valid,\n",
    "            invalid=invalid,\n",
    "            repairs=repairs,\n",
    "            errors=errors,\n",
    "            performed_writes=[],\n",
    "            performed_skips=[],\n",
    "        )\n",
    "\n",
    "    # Dry-run: print plan and return\n",
    "    if dry_run:\n",
    "        for r in repairs:\n",
    "            print(f\"[plan] {r['from']} -> {r['to']}\")\n",
    "        print(f\"[dry-run] {len(repairs)} rename(s) would be performed; {len(valid)} already ok.\")\n",
    "        params = dict(params_base)\n",
    "        return OperationLog(\n",
    "            params=params,\n",
    "            candidates=candidates,\n",
    "            excluded=excluded,\n",
    "            valid=valid,\n",
    "            invalid=invalid,\n",
    "            repairs=repairs,\n",
    "            errors=[],\n",
    "            performed_writes=[],\n",
    "            performed_skips=[],\n",
    "        )\n",
    "\n",
    "    # Perform writes\n",
    "    performed_writes: List[str] = []\n",
    "    performed_skips: List[str] = []\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def desired_metadata_name(new_base: str, new_ext: str) -> str:\n",
    "        return f\"{new_base}{new_ext}\" if metadata_name_includes_extension else new_base\n",
    "\n",
    "    # Optionally copy valid files unchanged (to mirror the whole corpus)\n",
    "    if copy_valid_when_writing:\n",
    "        for rel_str in valid:\n",
    "            src = input_dir / rel_str\n",
    "            dst = output_dir / rel_str\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(src, \"rb\") as fsrc, open(dst, \"wb\") as fdst:\n",
    "                fdst.write(fsrc.read())\n",
    "            performed_skips.append(rel_str)\n",
    "\n",
    "    # Write repaired files with updated metadata.name\n",
    "    for r in repairs:\n",
    "        orig_rel = r[\"from\"]\n",
    "        new_rel = r[\"to\"]\n",
    "        src = input_dir / orig_rel\n",
    "        dst = output_dir / new_rel\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(src, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Failed to parse JSON: {orig_rel}: {e}\") from e\n",
    "\n",
    "        # Expect data[\"metadata\"][\"name\"]\n",
    "        try:\n",
    "            md = data[\"metadata\"]\n",
    "            _ = md[\"name\"]\n",
    "        except Exception:\n",
    "            raise KeyError(f'missing data[\"metadata\"][\"name\"] in {orig_rel}')\n",
    "\n",
    "        md[\"name\"] = desired_metadata_name(r[\"new_basename\"], r[\"new_ext\"])\n",
    "\n",
    "        with open(dst, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        print(f\"[write] {orig_rel} -> {new_rel} (metadata.name={md['name']})\")\n",
    "        performed_writes.append(new_rel)\n",
    "\n",
    "    # Write CHANGES.tab (TSV) with planned repairs\n",
    "    changes_path = output_dir / \"CHANGES.tab\"\n",
    "    with open(changes_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        f.write(\"from_rel\\tto_rel\\told_basename\\tnew_basename\\told_ext\\tnew_ext\\n\")\n",
    "        for r in repairs:\n",
    "            f.write(\n",
    "                f\"{r['from']}\\t{r['to']}\\t\"\n",
    "                f\"{r['old_basename']}\\t{r['new_basename']}\\t\"\n",
    "                f\"{r['old_ext']}\\t{r['new_ext']}\\n\"\n",
    "            )\n",
    "    print(f\"[changes] wrote {changes_path}\")\n",
    "\n",
    "    print(f\"[done] wrote {len(performed_writes)} file(s); \"\n",
    "          f\"copied {len(performed_skips)} unchanged (copy_valid_when_writing={copy_valid_when_writing}).\")\n",
    "\n",
    "    params = dict(params_base)\n",
    "    params[\"changes_tsv\"] = str(changes_path)\n",
    "\n",
    "    return OperationLog(\n",
    "        params=params,\n",
    "        candidates=candidates,\n",
    "        excluded=excluded,\n",
    "        valid=valid,\n",
    "        invalid=invalid,\n",
    "        repairs=repairs,\n",
    "        errors=[],\n",
    "        performed_writes=performed_writes,\n",
    "        performed_skips=performed_skips,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_ROOT = DEV_OUTPUT / \"cleaned_corpus\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input dir: {CORPORA_ROOT}\")\n",
    "print(f\"Output dir: {OUTPUT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d47019",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_log = rename_and_fix_json_files(\n",
    "    CORPORA_ROOT,\n",
    "    OUTPUT_ROOT,\n",
    "    dry_run=True,  # Set to False to perform writes\n",
    "    metadata_name_includes_extension=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_log = rename_and_fix_json_files(\n",
    "    CORPORA_ROOT,\n",
    "    OUTPUT_ROOT,\n",
    "    dry_run=False,\n",
    "    metadata_name_includes_extension=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9c23e",
   "metadata": {},
   "source": [
    "## Story Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2fb5a",
   "metadata": {},
   "source": [
    "Reload the corpus as we may have changed it from the earlier load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116af385",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stories = corpus_surveyor.find_corpus_stories(CORPORA_ROOT)\n",
    "len(json_stories)\n",
    "print(utils.sml(json_stories))\n",
    "print(\"Type of path element:\", type(json_stories[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_story = json_stories[0]\n",
    "print(\"Sample story path:\", sample_story)\n",
    "with open(sample_story, \"r\", encoding=\"utf-8\") as f:\n",
    "    sample_data = json.load(f)\n",
    "print(\"Sample story metadata:\", sample_data[\"metadata\"])\n",
    "sample_body = sample_data[\"body\"]\n",
    "print(f\"Sample story text length: {len(sample_body)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2aa111",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_classifier = story_analysis.make_doc_classification_extractor(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c90188",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = story_classifier(sample_body)\n",
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71120372",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = random.sample(json_stories, 100)\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = corpus_surveyor.process_files(\n",
    "    random_sample,\n",
    "    corpora_root=CORPORA_ROOT,\n",
    "    output_root=DEV_OUTPUT,\n",
    "    processor_function=story_classifier,\n",
    "    job_label=\"story_classes\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_summary = corpus_surveyor.process_files(\n",
    "    json_stories,\n",
    "    corpora_root=CORPORA_ROOT,\n",
    "    output_root=DEV_OUTPUT,\n",
    "    processor_function=story_classifier,\n",
    "    job_label=\"story_classes\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beefb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stories = [\n",
    "    CORPORA_ROOT / 'mass_quantities/george_walker_at_suez.json',\n",
    "    CORPORA_ROOT / 'mass_quantities/give_back_a_world.json',\n",
    "]\n",
    "\n",
    "missing_summary = corpus_surveyor.process_files(\n",
    "    missing_stories,\n",
    "    corpora_root=CORPORA_ROOT,\n",
    "    output_root=DEV_OUTPUT,\n",
    "    processor_function=story_classifier,\n",
    "    job_label=\"story_classes\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30816418",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e50db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f536d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
